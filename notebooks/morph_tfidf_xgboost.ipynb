{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7f4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize_morphs(text):\n",
    "    return \" \".join(okt.morphs(str(text)))\n",
    "\n",
    "def extract_features(text):\n",
    "    text = str(text)\n",
    "    features = {\n",
    "        \"length_chars\": len(text),\n",
    "        \"length_words\": len(text.split()),\n",
    "        \"num_commas\": text.count(\",\"),\n",
    "        \"num_periods\": text.count(\".\"),\n",
    "        \"avg_word_len\": sum(len(w) for w in text.split()) / len(text.split()) if text.split() else 0,\n",
    "        \"num_uppercase\": sum(c.isupper() for c in text),\n",
    "        \"num_digits\": sum(c.isdigit() for c in text),\n",
    "        \"num_punctuations\": len(re.findall(r'[^\\w\\s]', text)),\n",
    "    }\n",
    "    return pd.Series(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95057bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜•íƒœì†Œ ë¶„ì„ ì¤‘...\n",
      "TF-IDF ë²¡í„°í™” ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"open/train.csv\")\n",
    "\n",
    "print(\"í˜•íƒœì†Œ ë¶„ì„ ì¤‘...\")\n",
    "df[\"full_text_morph\"] = df[\"full_text\"].apply(tokenize_morphs)\n",
    "df_features = df[\"full_text\"].apply(extract_features)\n",
    "\n",
    "print(\"TF-IDF ë²¡í„°í™” ì¤‘...\")\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X_tfidf = vectorizer.fit_transform(df[\"full_text_morph\"])\n",
    "\n",
    "X_all = pd.concat([pd.DataFrame(X_tfidf.toarray()), df_features.reset_index(drop=True)], axis=1)\n",
    "y_all = df[\"generated\"]\n",
    "\n",
    "# ğŸ”¹ AI:ì‚¬ëŒ = 1:2 ë¹„ìœ¨ ìƒ˜í”Œë§\n",
    "df_ai = X_all[df[\"generated\"] == 1]\n",
    "df_human = X_all[df[\"generated\"] == 0].sample(n=len(df_ai)*2, random_state=42)\n",
    "X_balanced = pd.concat([df_ai, df_human])\n",
    "y_balanced = pd.Series([1]*len(df_ai) + [0]*len(df_human))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c32e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "âœ… Validation ROC AUC: 0.93042\n"
     ]
    }
   ],
   "source": [
    "print(\"ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "roc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "print(\"âœ… Validation ROC AUC:\", round(roc_score, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2593b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ì…‹ í˜•íƒœì†Œ ë¶„ì„ ì¤‘...\n",
      "í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ì¤‘...\n",
      "ğŸ‰ ì œì¶œ ì™„ë£Œ: submission_best_tfidf_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"open/test.csv\")\n",
    "test_df = test_df.rename(columns={\"paragraph_text\": \"full_text\"})\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ì…‹ í˜•íƒœì†Œ ë¶„ì„ ì¤‘...\")\n",
    "test_df[\"full_text_morph\"] = test_df[\"full_text\"].apply(tokenize_morphs)\n",
    "test_features = test_df[\"full_text\"].apply(extract_features)\n",
    "test_tfidf = vectorizer.transform(test_df[\"full_text_morph\"])\n",
    "test_all = pd.concat([pd.DataFrame(test_tfidf.toarray()), test_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ì¤‘...\")\n",
    "test_probs = model.predict_proba(test_all)[:, 1]\n",
    "\n",
    "submission = pd.read_csv(\"open/sample_submission.csv\")\n",
    "submission[\"generated\"] = test_probs\n",
    "submission.to_csv(\"submission_best_tfidf_tuned.csv\", index=False)\n",
    "print(\"ğŸ‰ ì œì¶œ ì™„ë£Œ: submission_best_tfidf_tuned.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
